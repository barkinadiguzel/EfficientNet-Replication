# ğŸ° EfficientNet-B0-Replication PyTorch Implementation

This repository contains a replication of **EfficientNet-B0** using PyTorch, following the **compound scaling** and **MBConv** concepts from the original paper. The model is designed for **efficient image classification** while maintaining a good trade-off between **accuracy and FLOPs**.

- Implemented **EfficientNet-B0** with **MBConv blocks**, **depthwise separable convolutions**, and **Squeeze-and-Excitation (SE) layers**.  
- Architecture:  
**Stem â†’ MBConv Stages â†’ Conv1x1 + GlobalAvgPool â†’ Flatten â†’ FC**

> **Note on EfficientNet:** The model uses **compound scaling**, which uniformly scales network **width, depth, and resolution** using fixed coefficients derived from a small grid search on the baseline network.

**Paper reference:** [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) âœ¨

---

## ğŸ–¼ Overview â€“ EfficientNet-B0 Architecture

![MBConv & Compoundâ€‘Scaling Diagram](images/mbconv_diagram.jpg)

- *Figure 2* shows different scaling strategies: baseline network, widthâ€‘only, depthâ€‘only, resolutionâ€‘only, and the proposed **compound scaling** (scaling all three together with fixed ratios). 
- *Figure 3* demonstrates the effect of scaling single dimension vs compound scaling â€” authors observe that scaling only width or only depth or only resolution yields diminishing returns for large models.
- *Table 1* presents the **EfficientNet-B0 baseline** configuration: for each stage it gives operator type (Conv3Ã—3 or MBConv), kernel size, number of channels, number of layers, and feature map resolution. 

> **Why this design matters:**  
> - Using **MBConv blocks + Squeezeâ€‘andâ€‘Excitation (SE)** lets network be efficient but still powerful â€” good balance of parameters/FLOPs vs performance. 
> - The **compound scaling rule** balances depth, width and resolution so you donâ€™t blow up FLOPs or memory for little accuracy gain.  
> - The baseline B0 architecture (stem â†’ MBConv stages â†’ head + pooling + classifier) is chosen via smallâ€‘scale neuralâ€‘architectureâ€‘search to optimize efficiency under a FLOPs budget.

---

## ğŸ§® Mathematical Concepts

### Depthwise Separable Convolution

$$
y = Conv_{1x1}(DWConv_{kxk}(x))
$$

- **x**: input tensor  
- **DWConv**: depthwise convolution (channel-wise spatial filtering)  
- **Conv1x1**: pointwise convolution to mix channels efficiently

### Compound Scaling

$$
d = \alpha^\phi, \quad w = \beta^\phi, \quad r = \gamma^\phi
$$

- **d, w, r**: depth, width, resolution scaling coefficients  
- **Î±, Î², Î³**: constants determined by grid search on baseline B0  
- **Ï†**: user-specified scaling factor  
- Scales network uniformly while maintaining computational efficiency

> Scaling only one dimension (depth, width, or resolution) saturates accuracy. Compound scaling balances all three dimensions for optimal performance.

---

## ğŸ—ï¸ Model Architecture

```bash
EfficientNet-B0-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py             # Basic convolution layer
â”‚   â”‚   â”œâ”€â”€ depthwise_conv.py         # Depthwise convolution helper
â”‚   â”‚   â”œâ”€â”€ se_layer.py               # Squeeze-and-Excitation block
â”‚   â”‚   â”œâ”€â”€ pooling_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ avgpool_layer.py      # Global/Adaptive average pooling
â”‚   â”‚   â”‚   â””â”€â”€ maxpool_layer.py      # Max pooling
â”‚   â”‚   â”œâ”€â”€ flatten.py                # Flatten layer
â”‚   â”‚   â””â”€â”€ fc.py                     # Fully connected classifier
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ mbconv_block.py           # MBConv block (depthwise + pointwise + SE + skip)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ efficientnet_b0.py        # Stem + MBConv stages + Head (1x1 conv + pool + FC)
â”‚   â”‚
â”‚   â””â”€â”€ config.py                     # Stage configuration, input size, width/depth multipliers
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ mbconv_diagram.jpg            # MBConv block diagram (Figures 2, 3 + Table 1) ğŸ­
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
